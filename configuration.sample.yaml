general:
  random_seed: 100
  directory_template: "b{ct}-{earc}-{darc}-{epc}"
  output_directory_template: "./models/{dir}"

logger:
  log_format: "(%(asctime)s) %(name)s [%(levelname)s]: %(message)s"
  log_file: "./logs/{dir}/seq2seq.log"
  log_level: "info"

export:
  csv:
    svc:
      - "Kernel"
      - "Nu"
      - "Gamma"
      - "F1"
    knc:
      - "Precision"
      - "Recall"
      - "F1"

data:
  reading:
    user_count: 40
    genuine_sample_count: 20
    forged_sample_count: 20
    signature_path_template: "./data_set/U{user}S{sample}.TXT"
    feature_count: 4

  reshaping:
    input_dimension: 27
    sampling_step: 3

classifiers:
  reference_sample_count: 5
  train_user_count: 32
  test_user_count: 8

autoencoder:
  train:
    batch_size: 32
    epochs: 100
    verbose: 1

  compile_config:
    loss: "mape"
    optimizer: "nadam"
    metrics:
      - "acc"

  callbacks:
    tensorboard:
      log_dir_template: "./logs/{dir}/tensorboard"
    early_stopping:
      monitor: "loss"
      min_delta: 0
      patience: 10
      verbose: 1
      mode: "auto"

  architecture:
    global: &global
      return_sequences: true
      implementation: 2
      dropout: 0.4
    encoder:
      0:
        <<: *global
        units: 72
      1:
        <<: *global
        units: 150
    decoder:
      0:
        <<: *global
        units: 9
    cell_type: "LSTM"
