general:
  random_seed: 100
  directory_template: "b{ct}-{earc}-{darc}-{epc}"
  output_directory_template: "./models/{dir}"

logger:
  log_format: "(%(asctime)s) %(name)s [%(levelname)s]: %(message)s"
  log_file: "./models/{dir}/seq2seq.log"
  log_level: "info"

export:
  csv:
    svc:
      - "Kernel"
      - "Nu"
      - "Gamma"
      - "F1"
    knc:
      - "Precision"
      - "Recall"
      - "F1"

data:
  reading:
    writer_count: 10000
    genuine_sample_count: 24
    forged_sample_count: 30
    dataset_path: "dataset.npz"

  reshaping:
    input_dimension: 14
    sampling_step: 2
    window_size: 1
    window_step: 1
    length_threshold: 30000

classifiers:
  reference_sample_count: 5
  train_writer_count: 8000
  test_writer_count: 2000
  knc:
    n_neighbors: 45
  svc:
    nu:
      start: 0.500
      stop: 0.850
      step: 0.001
    gamma:
      - 1.0e-1
      - 2.0e-1
      - 3.0e-1
      - 1.0e-2
      - 2.0e-2
      - 3.0e-2
      - 1.0e-3
      - 2.0e-3
      - 3.0e-3
      - 1.0e-4
      - 2.0e-4
      - 3.0e-4
      - 1.0e-5
      - 2.0e-5
      - 3.0e-5
      - 1.0e-6
      - 2.0e-6
      - 3.0e-6
      - 1.0e-7
      - 2.0e-7
      - 3.0e-7
      - 1.0e-8
      - 2.0e-8
      - 3.0e-8
      - 1.0e-9
      - 2.0e-9
      - 3.0e-9
      - 1.0e-10
      - 2.0e-10
      - 3.0e-10

autoencoder:
  train:
    batch_size: 128
    epochs: 1000
    validation_split: 0.0
    verbose: 1

  compile_config:
    loss: "mape"
    optimizer:
      name: "nadam"
      args: {}
    metrics:
      - "mae"
      - "mse"

  callbacks:
    early_stopping:
      monitor: "loss"
      patience: 10
      verbose: 1

  architecture:
    global: &global
      return_sequences: true
      implementation: 2
      activation: "relu"
      dropout: 0.2
      merge_mode: "concat"

    encoder:
      0:
        <<: *global
        go_backwards: true
        units: 100

    decoder:
      0:
        <<: *global
        units: 50
      1:
        <<: *global
        merge_mode: "ave"
        units: 14

    cell_type: "LSTM"
